{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 14, "column": 0}, "map": {"version":3,"sources":["file:///D:/Courses%20LOL/Codes/portfolio-website-build/app/projects/legal-led-summarization/page.tsx"],"sourcesContent":["export default function LegalLEDBlog() {\r\n  return (\r\n    <article className=\"max-w-4xl mx-auto px-6 py-24 space-y-14 font-mono\">\r\n      {/* ================= Header ================= */}\r\n      <header className=\"space-y-4\">\r\n        <h1 className=\"text-4xl md:text-5xl font-bold text-cyan-400\">\r\n          Engineering Long-Document Legal Summarization with LED\r\n        </h1>\r\n        <p className=\"text-gray-400 text-lg\">\r\n          From long-context model training to real-world deployment constraints.\r\n        </p>\r\n        <div className=\"h-[2px] w-32 bg-gradient-to-r from-cyan-400 to-purple-400\" />\r\n      </header>\r\n\r\n      {/* ================= Overview ================= */}\r\n      <section className=\"space-y-3\">\r\n        <h2 className=\"text-2xl text-purple-400 font-semibold\">Overview</h2>\r\n        <p className=\"text-gray-300 leading-relaxed\">\r\n          This project focuses on abstractive summarization of long legal\r\n          documents such as legislative bills, court judgments, and hearing\r\n          transcripts using the Longformer Encoder-Decoder (LED) architecture.\r\n        </p>\r\n        <p className=\"text-gray-300 leading-relaxed\">\r\n          The model was trained and evaluated on the{\" \"}\r\n          <span className=\"text-cyan-400\">BillSum</span> dataset, which consists\r\n          of U.S. Congressional bills published between approximately{\" \"}\r\n          <span className=\"text-cyan-400\">2010 and 2017</span>. These documents\r\n          are structurally complex, legally dense, and often span tens of\r\n          thousands of tokens.\r\n        </p>\r\n        <p className=\"text-gray-300 leading-relaxed\">\r\n          The primary objective was not just to train a summarization model, but\r\n          to engineer a{\" \"}\r\n          <span className=\"text-cyan-400\">\r\n            robust, deployable long-context NLP system{\" \"}\r\n          </span>\r\n          capable of handling real-world legal inputs under practical memory,\r\n          latency, and cost constraints.\r\n        </p>\r\n      </section>\r\n\r\n      {/* ================= Why Hard ================= */}\r\n      <section className=\"space-y-3\">\r\n        <h2 className=\"text-2xl text-purple-400 font-semibold\">\r\n          Why Legal Document Summarization Is Hard\r\n        </h2>\r\n        <ul className=\"list-disc list-inside text-gray-300 space-y-2\">\r\n          <li>\r\n            Legal documents frequently exceed standard transformer token limits\r\n          </li>\r\n          <li>Strong long-range and cross-section dependencies</li>\r\n          <li>Numerical, citation-heavy clauses where precision is critical</li>\r\n          <li>\r\n            Non-digitally native PDFs introducing OCR and formatting noise\r\n          </li>\r\n        </ul>\r\n      </section>\r\n\r\n      {/* ================= Model Choice ================= */}\r\n      <section className=\"space-y-4\">\r\n        <h2 className=\"text-2xl text-purple-400 font-semibold\">\r\n          Model Selection: Why LED\r\n        </h2>\r\n\r\n        <div className=\"space-y-2 text-gray-300 leading-relaxed\">\r\n          <p>\r\n            <span className=\"text-cyan-400\">BERT</span> is an encoder-only model\r\n            primarily suited for extractive tasks and is not designed for\r\n            long-form text generation.\r\n          </p>\r\n          <p>\r\n            <span className=\"text-cyan-400\">T5</span> supports abstractive\r\n            summarization but relies on full self-attention, which becomes\r\n            computationally infeasible for very long legal documents.\r\n          </p>\r\n          <p>\r\n            <span className=\"text-cyan-400\">LED</span> replaces full attention\r\n            with sliding-window attention and introduces global attention\r\n            tokens, making it well-suited for long-context abstractive\r\n            summarization tasks such as legislative bill analysis.\r\n          </p>\r\n        </div>\r\n      </section>\r\n\r\n      {/* ================= Limiting Constraints ================= */}\r\n      <section className=\"space-y-3\">\r\n        <h2 className=\"text-2xl text-purple-400 font-semibold\">\r\n          Training Constraints & Engineering Workarounds\r\n        </h2>\r\n\r\n        <p className=\"text-gray-300 leading-relaxed\">\r\n          One of the major challenges in this project was fine-tuning\r\n          <span className=\"text-cyan-400\"> LED-Large (~460M parameters){\" \"}</span>\r\n          under realistic compute constraints.\r\n        </p>\r\n\r\n        <p className=\"text-gray-300 leading-relaxed\">\r\n          Full fine-tuning of LED-Large with long input sequences (8kâ€“16k\r\n          tokens) is impractical on laptops and free-tier cloud environments\r\n          such as Google Colab due to severe GPU memory limitations. Even GPUs\r\n          like T4 and P100 struggle once optimizer states, gradients, and\r\n          long-sequence activations are taken into account.\r\n        </p>\r\n\r\n        <p className=\"text-gray-300 leading-relaxed\">\r\n          To address this, the model was fine-tuned using\r\n          <span className=\"text-cyan-400\">\r\n            {\" \"}\r\n            QLoRA (Quantized Low-Rank Adaptation)\r\n          </span>\r\n          , which enables efficient adaptation of large models by training a\r\n          small number of low-rank parameters on top of a quantized base model.\r\n        </p>\r\n\r\n        <p className=\"text-gray-300 leading-relaxed\">\r\n          Training was conducted on Kaggle GPUs (T4/P100), which offer extended\r\n          compute availability compared to typical free-tier environments. QLoRA\r\n          made it possible to fine-tune the model within memory limits while\r\n          retaining performance close to full fine-tuning.\r\n        </p>\r\n\r\n        <p className=\"text-gray-300 leading-relaxed\">\r\n          While parameter-efficient fine-tuning may result in a slight\r\n          performance gap compared to full fine-tuning, the trade-off was\r\n          justified given the significant reductions in memory usage and\r\n          training cost.\r\n        </p>\r\n      </section>\r\n\r\n      {/* ================= Attention ================= */}\r\n      <section className=\"space-y-3\">\r\n        <h2 className=\"text-2xl text-purple-400 font-semibold\">\r\n          Attention Design & Global Context Handling\r\n        </h2>\r\n        <p className=\"text-gray-300 leading-relaxed\">\r\n          Sliding-window attention efficiently captures local context but lacks\r\n          document-level awareness. LED addresses this through global attention\r\n          tokens that can attend to the entire sequence.\r\n        </p>\r\n        <p className=\"text-gray-300 leading-relaxed\">\r\n          In this implementation, global attention was applied dynamically by\r\n          updating global attention anchors approximately every\r\n          <span className=\"text-cyan-400\"> 1000 tokens</span>. This ensures that\r\n          different sections of long legal documents receive global context\r\n          coverage, reducing topic drift and improving coherence across\r\n          sections.\r\n        </p>\r\n      </section>\r\n\r\n      {/* ================= Chunking ================= */}\r\n      <section className=\"space-y-3\">\r\n        <h2 className=\"text-2xl text-purple-400 font-semibold\">\r\n          Token Limits & Chunking Strategy\r\n        </h2>\r\n        <p className=\"text-gray-300 leading-relaxed\">\r\n          While{\" \"}\r\n          <span className=\"text-cyan-400\">\r\n            LED-Large supports up to 16k tokens\r\n          </span>\r\n          , many real-world legal documents exceed this limit. To handle such\r\n          cases, documents were split into overlapping chunks.\r\n        </p>\r\n        <p className=\"text-gray-300 leading-relaxed\">\r\n          Each chunk is summarized independently, with overlap preserving\r\n          semantic continuity across boundaries. This approach prioritizes\r\n          scalability and system stability over perfect global coherence.\r\n        </p>\r\n      </section>\r\n\r\n      {/* ================= OCR ================= */}\r\n      <section className=\"space-y-3\">\r\n        <h2 className=\"text-2xl text-purple-400 font-semibold\">\r\n          OCR & PDF Data Quality Challenges\r\n        </h2>\r\n        <ul className=\"list-disc list-inside text-gray-300 space-y-2\">\r\n          <li>Page numbers and headers injected into content</li>\r\n          <li>Logos, symbols, and formatting artifacts</li>\r\n          <li>Broken words and inconsistent spacing</li>\r\n        </ul>\r\n        <p className=\"text-gray-300 leading-relaxed\">\r\n          While the BillSum dataset itself is clean, real-world PDFs introduce\r\n          significant noise. Cleaning such artifacts using regular expressions\r\n          is unreliable and often incomplete, sometimes degrading summary\r\n          quality. As a result, the system emphasizes model robustness over\r\n          aggressive preprocessing.\r\n        </p>\r\n      </section>\r\n\r\n      {/* ================= Evaluation ================= */}\r\n      <section className=\"space-y-3\">\r\n        <h2 className=\"text-2xl text-purple-400 font-semibold\">\r\n          Evaluation Strategy\r\n        </h2>\r\n        <p className=\"text-gray-300 leading-relaxed\">\r\n          Evaluation was performed using multiple complementary metrics:\r\n        </p>\r\n        <ul className=\"list-disc list-inside text-gray-300 space-y-2\">\r\n          <li>\r\n            <span className=\"text-cyan-400\">ROUGE-1</span>: unigram overlap\r\n          </li>\r\n          <li>\r\n            <span className=\"text-cyan-400\">ROUGE-2</span>: bigram overlap\r\n          </li>\r\n          <li>\r\n            <span className=\"text-cyan-400\">ROUGE-L</span>: longest common\r\n            subsequence\r\n          </li>\r\n          <li>\r\n            <span className=\"text-cyan-400\">BERTScore</span>: semantic\r\n            similarity\r\n          </li>\r\n          <li>\r\n            <span className=\"text-cyan-400\">METEOR</span>: alignment-based\r\n            evaluation\r\n          </li>\r\n        </ul>\r\n        <p className=\"text-gray-300 leading-relaxed\">\r\n          Using multiple metrics provides a more realistic assessment of both\r\n          lexical fidelity and semantic faithfulness.\r\n        </p>\r\n      </section>\r\n\r\n      {/* ================= Limitations ================= */}\r\n      <section className=\"space-y-3\">\r\n        <h2 className=\"text-2xl text-purple-400 font-semibold\">\r\n          Limitations & Improvement Zones\r\n        </h2>\r\n        <ul className=\"list-disc list-inside text-gray-300 space-y-2\">\r\n          <li>Hallucinations under heavy OCR noise</li>\r\n          <li>Weak handling of dense numerical clauses</li>\r\n          <li>Sensitivity to chunk boundary placement</li>\r\n          <li>Limited cross-chunk reasoning</li>\r\n        </ul>\r\n      </section>\r\n\r\n      {/* ================= Deployment ================= */}\r\n      <section className=\"space-y-3\">\r\n        <h2 className=\"text-2xl text-purple-400 font-semibold\">\r\n          Deployment & System Design\r\n        </h2>\r\n        <p className=\"text-gray-300 leading-relaxed\">\r\n          The system is deployed using a FastAPI backend for inference, with the\r\n          model hosted on Hugging Face Spaces and the frontend built using\r\n          Next.js and deployed on Vercel.\r\n        </p>\r\n        <p className=\"text-gray-300 leading-relaxed\">\r\n          AWS deployment was explored; however, the free tier is insufficient\r\n          for hosting large models such as LED due to memory and compute\r\n          limitations. Hugging Face Spaces provided a more practical and\r\n          cost-effective solution for hosting long-context models.\r\n        </p>\r\n      </section>\r\n\r\n      {/* ================= Takeaway ================= */}\r\n      <section className=\"space-y-3\">\r\n        <h2 className=\"text-2xl text-purple-400 font-semibold\">Key Takeaway</h2>\r\n        <p className=\"text-gray-300 leading-relaxed\">\r\n          Building long-context NLP systems in production is fundamentally an\r\n          engineering challenge. This project highlights the trade-offs between\r\n          model capacity, context length, data quality, deployment cost, and\r\n          real-world robustness, far beyond simply training a high-performing\r\n          model.\r\n        </p>\r\n      </section>\r\n    </article>\r\n  );\r\n}\r\n"],"names":[],"mappings":";;;;;;AAAe,SAAS;IACtB,qBACE,8OAAC;QAAQ,WAAU;;0BAEjB,8OAAC;gBAAO,WAAU;;kCAChB,8OAAC;wBAAG,WAAU;kCAA+C;;;;;;kCAG7D,8OAAC;wBAAE,WAAU;kCAAwB;;;;;;kCAGrC,8OAAC;wBAAI,WAAU;;;;;;;;;;;;0BAIjB,8OAAC;gBAAQ,WAAU;;kCACjB,8OAAC;wBAAG,WAAU;kCAAyC;;;;;;kCACvD,8OAAC;wBAAE,WAAU;kCAAgC;;;;;;kCAK7C,8OAAC;wBAAE,WAAU;;4BAAgC;4BACA;0CAC3C,8OAAC;gCAAK,WAAU;0CAAgB;;;;;;4BAAc;4BACc;0CAC5D,8OAAC;gCAAK,WAAU;0CAAgB;;;;;;4BAAoB;;;;;;;kCAItD,8OAAC;wBAAE,WAAU;;4BAAgC;4BAE7B;0CACd,8OAAC;gCAAK,WAAU;;oCAAgB;oCACa;;;;;;;4BACtC;;;;;;;;;;;;;0BAOX,8OAAC;gBAAQ,WAAU;;kCACjB,8OAAC;wBAAG,WAAU;kCAAyC;;;;;;kCAGvD,8OAAC;wBAAG,WAAU;;0CACZ,8OAAC;0CAAG;;;;;;0CAGJ,8OAAC;0CAAG;;;;;;0CACJ,8OAAC;0CAAG;;;;;;0CACJ,8OAAC;0CAAG;;;;;;;;;;;;;;;;;;0BAOR,8OAAC;gBAAQ,WAAU;;kCACjB,8OAAC;wBAAG,WAAU;kCAAyC;;;;;;kCAIvD,8OAAC;wBAAI,WAAU;;0CACb,8OAAC;;kDACC,8OAAC;wCAAK,WAAU;kDAAgB;;;;;;oCAAW;;;;;;;0CAI7C,8OAAC;;kDACC,8OAAC;wCAAK,WAAU;kDAAgB;;;;;;oCAAS;;;;;;;0CAI3C,8OAAC;;kDACC,8OAAC;wCAAK,WAAU;kDAAgB;;;;;;oCAAU;;;;;;;;;;;;;;;;;;;0BAShD,8OAAC;gBAAQ,WAAU;;kCACjB,8OAAC;wBAAG,WAAU;kCAAyC;;;;;;kCAIvD,8OAAC;wBAAE,WAAU;;4BAAgC;0CAE3C,8OAAC;gCAAK,WAAU;;oCAAgB;oCAA8B;;;;;;;4BAAW;;;;;;;kCAI3E,8OAAC;wBAAE,WAAU;kCAAgC;;;;;;kCAQ7C,8OAAC;wBAAE,WAAU;;4BAAgC;0CAE3C,8OAAC;gCAAK,WAAU;;oCACb;oCAAI;;;;;;;4BAEA;;;;;;;kCAKT,8OAAC;wBAAE,WAAU;kCAAgC;;;;;;kCAO7C,8OAAC;wBAAE,WAAU;kCAAgC;;;;;;;;;;;;0BAS/C,8OAAC;gBAAQ,WAAU;;kCACjB,8OAAC;wBAAG,WAAU;kCAAyC;;;;;;kCAGvD,8OAAC;wBAAE,WAAU;kCAAgC;;;;;;kCAK7C,8OAAC;wBAAE,WAAU;;4BAAgC;0CAG3C,8OAAC;gCAAK,WAAU;0CAAgB;;;;;;4BAAmB;;;;;;;;;;;;;0BAQvD,8OAAC;gBAAQ,WAAU;;kCACjB,8OAAC;wBAAG,WAAU;kCAAyC;;;;;;kCAGvD,8OAAC;wBAAE,WAAU;;4BAAgC;4BACrC;0CACN,8OAAC;gCAAK,WAAU;0CAAgB;;;;;;4BAEzB;;;;;;;kCAIT,8OAAC;wBAAE,WAAU;kCAAgC;;;;;;;;;;;;0BAQ/C,8OAAC;gBAAQ,WAAU;;kCACjB,8OAAC;wBAAG,WAAU;kCAAyC;;;;;;kCAGvD,8OAAC;wBAAG,WAAU;;0CACZ,8OAAC;0CAAG;;;;;;0CACJ,8OAAC;0CAAG;;;;;;0CACJ,8OAAC;0CAAG;;;;;;;;;;;;kCAEN,8OAAC;wBAAE,WAAU;kCAAgC;;;;;;;;;;;;0BAU/C,8OAAC;gBAAQ,WAAU;;kCACjB,8OAAC;wBAAG,WAAU;kCAAyC;;;;;;kCAGvD,8OAAC;wBAAE,WAAU;kCAAgC;;;;;;kCAG7C,8OAAC;wBAAG,WAAU;;0CACZ,8OAAC;;kDACC,8OAAC;wCAAK,WAAU;kDAAgB;;;;;;oCAAc;;;;;;;0CAEhD,8OAAC;;kDACC,8OAAC;wCAAK,WAAU;kDAAgB;;;;;;oCAAc;;;;;;;0CAEhD,8OAAC;;kDACC,8OAAC;wCAAK,WAAU;kDAAgB;;;;;;oCAAc;;;;;;;0CAGhD,8OAAC;;kDACC,8OAAC;wCAAK,WAAU;kDAAgB;;;;;;oCAAgB;;;;;;;0CAGlD,8OAAC;;kDACC,8OAAC;wCAAK,WAAU;kDAAgB;;;;;;oCAAa;;;;;;;;;;;;;kCAIjD,8OAAC;wBAAE,WAAU;kCAAgC;;;;;;;;;;;;0BAO/C,8OAAC;gBAAQ,WAAU;;kCACjB,8OAAC;wBAAG,WAAU;kCAAyC;;;;;;kCAGvD,8OAAC;wBAAG,WAAU;;0CACZ,8OAAC;0CAAG;;;;;;0CACJ,8OAAC;0CAAG;;;;;;0CACJ,8OAAC;0CAAG;;;;;;0CACJ,8OAAC;0CAAG;;;;;;;;;;;;;;;;;;0BAKR,8OAAC;gBAAQ,WAAU;;kCACjB,8OAAC;wBAAG,WAAU;kCAAyC;;;;;;kCAGvD,8OAAC;wBAAE,WAAU;kCAAgC;;;;;;kCAK7C,8OAAC;wBAAE,WAAU;kCAAgC;;;;;;;;;;;;0BAS/C,8OAAC;gBAAQ,WAAU;;kCACjB,8OAAC;wBAAG,WAAU;kCAAyC;;;;;;kCACvD,8OAAC;wBAAE,WAAU;kCAAgC;;;;;;;;;;;;;;;;;;AAUrD"}}]
}